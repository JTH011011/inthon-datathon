[ckpt] Loading checkpoint from best_model.pt ...
[ckpt] Loaded model_state
[ckpt] Loaded optim_state
[ckpt] Loaded scheduler_state
[ckpt] Resuming from step 500
train:   2%|██▌                                                                                                     | 500/20000 [00:00<?, ?step/s]/opt/homebrew/Caskroom/miniconda/base/envs/inthon/lib/python3.10/site-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
Starting epoch 1/100
  warnings.warn(
train:   5%|████▊                                                                                          | 1000/20000 [01:10<41:59,  7.54step/s]Traceback (most recent call last):
  File "/Users/taeheejeong/Desktop/inthon-dataton/train.py", line 344, in <module>
    main()
  File "/Users/taeheejeong/Desktop/inthon-dataton/train.py", line 325, in main
    train_loop(
  File "/Users/taeheejeong/Desktop/inthon-dataton/train.py", line 171, in train_loop
    gen_ids = model.generate(
  File "/opt/homebrew/Caskroom/miniconda/base/envs/inthon/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
  File "/Users/taeheejeong/Desktop/inthon-dataton/model.py", line 339, in generate
    memory = self.transformer.encoder(src_emb, src_key_padding_mask=src_key_padding_mask)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/inthon/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/inthon/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/inthon/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 467, in forward
    ) and not torch._nested_tensor_from_mask_left_aligned(
NotImplementedError: The operator 'aten::_nested_tensor_from_mask_left_aligned' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 5811a8d7da873dd699ff6687092c225caffcf1bb. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.
